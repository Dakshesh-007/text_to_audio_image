{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cef03a04f6c742029808654774574b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8dfd2b105fc442f088656b08ec2e30c1",
              "IPY_MODEL_95b67b0367a44775a8e0e14595e31450",
              "IPY_MODEL_ebe91516d5a2401a811ee96a96d73605"
            ],
            "layout": "IPY_MODEL_a6e2d939746a400490f281c1b71fedb3"
          }
        },
        "8dfd2b105fc442f088656b08ec2e30c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28591d7b5b8e424599b2fc7ac2b76e3a",
            "placeholder": "​",
            "style": "IPY_MODEL_fe6e88c7b8d245a4be7bf66169ee1275",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "95b67b0367a44775a8e0e14595e31450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f205b033848640bcbf5cb8d0a88b3933",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84c8197f43cf437891a0b6a6e617541e",
            "value": 7
          }
        },
        "ebe91516d5a2401a811ee96a96d73605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_126621747cd841fe86a2a6ffd245f4ae",
            "placeholder": "​",
            "style": "IPY_MODEL_309e220b22834be5b600bdac078cce92",
            "value": " 7/7 [00:27&lt;00:00,  6.10s/it]"
          }
        },
        "a6e2d939746a400490f281c1b71fedb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28591d7b5b8e424599b2fc7ac2b76e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6e88c7b8d245a4be7bf66169ee1275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f205b033848640bcbf5cb8d0a88b3933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c8197f43cf437891a0b6a6e617541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "126621747cd841fe86a2a6ffd245f4ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309e220b22834be5b600bdac078cce92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch audiocraft transformers pyttsx3 gradio numpy scipy diffusers matplotlib librosa soundfile"
      ],
      "metadata": {
        "id": "Hl_vcZTHX7rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install --upgrade diffusers\n",
        "\n",
        "from audiocraft.models import MusicGen\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pyttsx3\n",
        "import gradio as gr\n",
        "from tempfile import NamedTemporaryFile\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize Models\n",
        "# MusicGen\n",
        "music_model = MusicGen.get_pretrained(\"small\", device=device)\n",
        "\n",
        "# GPT-2 for conversation\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "# Stable Diffusion for image generation\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Emotion detection for Text-to-Audio\n",
        "def get_emotion_tone(text):\n",
        "    if any(word in text.lower() for word in [\"happy\", \"joy\", \"excited\"]):\n",
        "        return \"happy\"\n",
        "    elif any(word in text.lower() for word in [\"sad\", \"down\", \"melancholy\"]):\n",
        "        return \"sad\"\n",
        "    elif any(word in text.lower() for word in [\"angry\", \"frustrated\"]):\n",
        "        return \"angry\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Image generation using Stable Diffusion\n",
        "def generate_image(prompt, style=\"realistic\"):\n",
        "    styled_prompt = f\"{style} style {prompt}\"\n",
        "    try:\n",
        "        image = pipe(styled_prompt).images[0]\n",
        "        temp_image = NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        image.save(temp_image.name)\n",
        "        return temp_image.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating image: {e}\"\n",
        "\n",
        "# Convert Text to Audio with Emotion\n",
        "def text_to_audio(text):\n",
        "    emotion = get_emotion_tone(text)\n",
        "    engine = pyttsx3.init()\n",
        "    engine.setProperty('rate', 150 if emotion == \"neutral\" else 180 if emotion == \"happy\" else 100 if emotion == \"sad\" else 200)\n",
        "    engine.setProperty('volume', 0.8 if emotion == \"neutral\" else 1.0 if emotion == \"happy\" or emotion == \"angry\" else 0.5)\n",
        "\n",
        "    temp_file = NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "    engine.save_to_file(text, temp_file.name)\n",
        "    engine.runAndWait()\n",
        "    return temp_file.name\n",
        "\n",
        "# Music generation using MusicGen\n",
        "def generate_music(prompt):\n",
        "    try:\n",
        "        descriptions = [prompt]\n",
        "        wav = music_model.generate(descriptions)\n",
        "        temp_file = NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        audio_data = wav.cpu().numpy()\n",
        "        wavfile.write(temp_file.name, music_model.sample_rate, audio_data[0, 0])\n",
        "        return temp_file.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating music: {e}\"\n",
        "\n",
        "# Spectrogram generation from audio\n",
        "def generate_spectrogram(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        S = librosa.feature.melspectrogram(y, sr=sr)\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap='coolwarm')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title('Mel-frequency spectrogram')\n",
        "        temp_image = NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        plt.savefig(temp_image.name)\n",
        "        plt.close()\n",
        "        return temp_image.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating spectrogram: {e}\"\n",
        "\n",
        "# Chat with AI (GPT-2)\n",
        "def chat_with_ai(user_input):\n",
        "    try:\n",
        "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\").to(device)\n",
        "        outputs = gpt2_model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error in chat generation: {e}\"\n",
        "\n",
        "# Simulate Video Generation using a Sequence of Images\n",
        "def generate_video(prompt):\n",
        "    frames = []\n",
        "    for i in range(5):  # Generate 5 frames as a sequence\n",
        "        frame_prompt = f\"{prompt} frame {i+1}\"\n",
        "        frame_path = generate_image(frame_prompt)\n",
        "        frames.append(Image.open(frame_path))\n",
        "\n",
        "    temp_video = NamedTemporaryFile(delete=False, suffix=\".gif\")\n",
        "    frames[0].save(temp_video.name, save_all=True, append_images=frames[1:], duration=500, loop=0)\n",
        "    return temp_video.name\n",
        "\n",
        "# Main interface logic\n",
        "def main_interface(input_text, task_type, style):\n",
        "    try:\n",
        "        if task_type == \"Conversation\":\n",
        "            response = chat_with_ai(input_text)\n",
        "            image_path = generate_image(f\"conversation about {input_text}\", style)\n",
        "            return response, None, image_path\n",
        "\n",
        "        elif task_type == \"Music\":\n",
        "            audio_path = generate_music(input_text)\n",
        "            spectrogram_path = generate_spectrogram(audio_path)\n",
        "            return \"Music Generated\", audio_path, spectrogram_path\n",
        "\n",
        "        elif task_type == \"Text to Audio\":\n",
        "            audio_path = text_to_audio(input_text)\n",
        "            image_path = generate_image(f\"text-to-audio conversion for {input_text}\", style)\n",
        "            return \"Audio Generated\", audio_path, image_path\n",
        "\n",
        "        elif task_type == \"Video Generation\":\n",
        "            video_path = generate_video(input_text)\n",
        "            audio_path = generate_music(input_text)\n",
        "            return \"Video Generated\", audio_path, video_path\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", None, None\n",
        "\n",
        "# Gradio interface setup\n",
        "interface = gr.Interface(\n",
        "    fn=main_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Text or Prompt\"),\n",
        "        gr.Radio([\"Conversation\", \"Music\", \"Text to Audio\", \"Video Generation\"], label=\"Select Task\"),\n",
        "        gr.Dropdown([\"realistic\", \"abstract\", \"comic\"], label=\"Select Style\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Generated Output\"),\n",
        "        gr.Audio(label=\"Generated Audio\", type=\"filepath\"),\n",
        "        gr.Image(label=\"Generated Image\", type=\"filepath\"),\n",
        "    ],\n",
        "    live=False,\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "yJYyklmMudoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#working code\n",
        "#!pip install transformers diffusers gradio librosa audiocraft pyttsx3\n",
        "#!pip install --upgrade torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "import torch\n",
        "from audiocraft.models import MusicGen\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pyttsx3\n",
        "import gradio as gr\n",
        "from tempfile import NamedTemporaryFile\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# MusicGen\n",
        "music_model = MusicGen.get_pretrained(\"small\", device=device)\n",
        "\n",
        "# GPT-2 for conversation\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "# Stable Diffusion for image generation\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Emotion detection for Text-to-Audio\n",
        "def get_emotion_tone(text):\n",
        "    if any(word in text.lower() for word in [\"happy\", \"joy\", \"excited\"]):\n",
        "        return \"happy\"\n",
        "    elif any(word in text.lower() for word in [\"sad\", \"down\", \"melancholy\"]):\n",
        "        return \"sad\"\n",
        "    elif any(word in text.lower() for word in [\"angry\", \"frustrated\"]):\n",
        "        return \"angry\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Image generation using Stable Diffusion\n",
        "def generate_image(prompt, style=\"realistic\"):\n",
        "    styled_prompt = f\"{style} style {prompt}\"\n",
        "    try:\n",
        "        image = pipe(styled_prompt).images[0]\n",
        "        temp_image = NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        image.save(temp_image.name)\n",
        "        return temp_image.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating image: {e}\"\n",
        "\n",
        "# Convert Text to Audio with Emotion\n",
        "def text_to_audio(text):\n",
        "    emotion = get_emotion_tone(text)\n",
        "    engine = pyttsx3.init()\n",
        "    engine.setProperty('rate', 150 if emotion == \"neutral\" else 180 if emotion == \"happy\" else 100 if emotion == \"sad\" else 200)\n",
        "    engine.setProperty('volume', 0.8 if emotion == \"neutral\" else 1.0 if emotion == \"happy\" or emotion == \"angry\" else 0.5)\n",
        "\n",
        "    temp_file = NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
        "    engine.save_to_file(text, temp_file.name)\n",
        "    engine.runAndWait()\n",
        "    return temp_file.name\n",
        "\n",
        "# Music generation using MusicGen\n",
        "def generate_music(prompt):\n",
        "    try:\n",
        "        descriptions = [prompt]\n",
        "        wav = music_model.generate(descriptions)\n",
        "        temp_file = NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "        audio_data = wav.cpu().numpy()\n",
        "        wavfile.write(temp_file.name, music_model.sample_rate, audio_data[0, 0])\n",
        "        return temp_file.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating music: {e}\"\n",
        "\n",
        "# Spectrogram generation from audio\n",
        "def generate_spectrogram(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        S = librosa.feature.melspectrogram(y, sr=sr)\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap='coolwarm')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title('Mel-frequency spectrogram')\n",
        "        temp_image = NamedTemporaryFile(delete=False, suffix=\".png\")\n",
        "        plt.savefig(temp_image.name)\n",
        "        plt.close()\n",
        "        return temp_image.name\n",
        "    except Exception as e:\n",
        "        return f\"Error generating spectrogram: {e}\"\n",
        "\n",
        "# Chat with AI (GPT-2)\n",
        "def chat_with_ai(user_input):\n",
        "    try:\n",
        "        inputs = tokenizer.encode(user_input, return_tensors=\"pt\").to(device)\n",
        "        outputs = gpt2_model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error in chat generation: {e}\"\n",
        "\n",
        "# Simulate Video Generation using a Sequence of Images\n",
        "def generate_video(prompt):\n",
        "    frames = []\n",
        "    for i in range(5):  # Generate 5 frames as a sequence\n",
        "        frame_prompt = f\"{prompt} frame {i+1}\"\n",
        "        frame_path = generate_image(frame_prompt)\n",
        "        frames.append(Image.open(frame_path))\n",
        "\n",
        "    temp_video = NamedTemporaryFile(delete=False, suffix=\".gif\")\n",
        "    frames[0].save(temp_video.name, save_all=True, append_images=frames[1:], duration=500, loop=0)\n",
        "    return temp_video.name\n",
        "\n",
        "# Main interface logic\n",
        "def main_interface(input_text, task_type, style):\n",
        "    try:\n",
        "        if task_type == \"Conversation\":\n",
        "            response = chat_with_ai(input_text)\n",
        "            image_path = generate_image(f\"conversation about {input_text}\", style)\n",
        "            return response, None, image_path\n",
        "\n",
        "        elif task_type == \"Music\":\n",
        "            audio_path = generate_music(input_text)\n",
        "            spectrogram_path = generate_spectrogram(audio_path)\n",
        "            return \"Music Generated\", audio_path, spectrogram_path\n",
        "\n",
        "        elif task_type == \"Text to Audio\":\n",
        "            audio_path = text_to_audio(input_text)\n",
        "            image_path = generate_image(f\"text-to-audio conversion for {input_text}\", style)\n",
        "            return \"Audio Generated\", audio_path, image_path\n",
        "\n",
        "        elif task_type == \"Video Generation\":\n",
        "            video_path = generate_video(input_text)\n",
        "            audio_path = generate_music(input_text)\n",
        "            return \"Video Generated\", audio_path, video_path\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\", None, None\n",
        "\n",
        "# Gradio interface setup\n",
        "interface = gr.Interface(\n",
        "    fn=main_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter Text or Prompt\"),\n",
        "        gr.Radio([\"Conversation\", \"Music\", \"Text to Audio\", \"Video Generation\"], label=\"Select Task\"),\n",
        "        gr.Dropdown([\"realistic\", \"abstract\", \"comic\"], label=\"Select Style\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Generated Output\"),\n",
        "        gr.Audio(label=\"Generated Audio\", type=\"filepath\"),\n",
        "        gr.Image(label=\"Generated Image\", type=\"filepath\"),\n",
        "    ],\n",
        "    live=False,\n",
        ")\n",
        "\n",
        "interface.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816,
          "referenced_widgets": [
            "cef03a04f6c742029808654774574b8f",
            "8dfd2b105fc442f088656b08ec2e30c1",
            "95b67b0367a44775a8e0e14595e31450",
            "ebe91516d5a2401a811ee96a96d73605",
            "a6e2d939746a400490f281c1b71fedb3",
            "28591d7b5b8e424599b2fc7ac2b76e3a",
            "fe6e88c7b8d245a4be7bf66169ee1275",
            "f205b033848640bcbf5cb8d0a88b3933",
            "84c8197f43cf437891a0b6a6e617541e",
            "126621747cd841fe86a2a6ffd245f4ae",
            "309e220b22834be5b600bdac078cce92"
          ]
        },
        "id": "NbpjKwY9zNgA",
        "outputId": "73d19ad5-a75e-4912-d48c-a22964df976e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/audiocraft/models/musicgen.py:80: UserWarning: MusicGen pretrained model relying on deprecated checkpoint mapping. Please use full pre-trained id instead: facebook/musicgen-small\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/audiocraft/models/loaders.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/audiocraft/models/loaders.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cef03a04f6c742029808654774574b8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f6806ba319a838d2bf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f6806ba319a838d2bf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}